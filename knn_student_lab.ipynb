{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": []},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {"cell_type":"markdown","metadata":{},"source":[
      "# kNN & Distance-Based Learning — Student Lab\n",
      "\n",
      "Complete all TODOs. Focus: vectorization, scaling, and failure modes." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "import numpy as np\n",
      "\n",
      "def check(name: str, cond: bool):\n",
      "    if not cond:\n",
      "        raise AssertionError(f'Failed: {name}')\n",
      "    print(f'OK: {name}')\n",
      "\n",
      "rng = np.random.default_rng(0)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 0 — Synthetic dataset generator\n",
      "We’ll generate Gaussian blobs with controllable dimension." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def make_blobs(n_train=400, n_test=200, d=2, sep=2.5):\n",
      "    # two Gaussian blobs\n",
      "    mu0 = np.zeros(d)\n",
      "    mu1 = np.zeros(d); mu1[0] = sep\n",
      "    X0 = rng.standard_normal((n_train//2, d)) + mu0\n",
      "    X1 = rng.standard_normal((n_train - n_train//2, d)) + mu1\n",
      "    X_train = np.vstack([X0, X1])\n",
      "    y_train = np.array([0]*len(X0) + [1]*len(X1))\n",
      "    perm = rng.permutation(n_train)\n",
      "    X_train, y_train = X_train[perm], y_train[perm]\n",
      "\n",
      "    T0 = rng.standard_normal((n_test//2, d)) + mu0\n",
      "    T1 = rng.standard_normal((n_test - n_test//2, d)) + mu1\n",
      "    X_test = np.vstack([T0, T1])\n",
      "    y_test = np.array([0]*len(T0) + [1]*len(T1))\n",
      "    perm = rng.permutation(n_test)\n",
      "    X_test, y_test = X_test[perm], y_test[perm]\n",
      "    return X_train, y_train, X_test, y_test\n",
      "\n",
      "X_train, y_train, X_test, y_test = make_blobs(d=5)\n",
      "check('shapes', X_train.shape[0] == y_train.shape[0] and X_test.shape[0] == y_test.shape[0])\n",
      "X_train.shape, X_test.shape"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 1 — Vectorized distances\n",
      "\n",
      "### Task 1.1: L2 distance matrix\n",
      "Compute D where D[i,j] = ||X_test[i] - X_train[j]||_2.\n",
      "\n",
      "# HINT:\n",
      "- Use expansion: ||a-b||^2 = ||a||^2 + ||b||^2 - 2 a·b\n",
      "- Avoid building (m,n,d) broadcast tensor for large sizes\n" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def l2_distances(X_test, X_train):\n",
      "    # TODO: return (m,n) matrix\n",
      "    ...\n",
      "\n",
      "D = l2_distances(X_test[:10], X_train[:20])\n",
      "check('D_shape', D.shape == (10, 20))\n",
      "check('D_nonneg', np.all(D >= -1e-9))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "### Task 1.2: L1 distances (optional)\n",
      "Compute L1 distance matrix (can use broadcasting here since we’ll keep sizes small).\n",
      "\n",
      "**Interview Angle:** When might L1 beat L2?" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def l1_distances(X_test, X_train):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "D1 = l1_distances(X_test[:5], X_train[:7])\n",
      "check('D1_shape', D1.shape == (5, 7))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 2 — kNN classifier\n",
      "\n",
      "### Task 2.1: Predict with kNN\n",
      "\n",
      "# HINT:\n",
      "- compute distances\n",
      "- argsort distances to find k nearest\n",
      "- majority vote\n",
      "\n",
      "**FAANG gotcha:** define tie-breaking deterministically (e.g., pick smallest label)." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def knn_predict(X_train, y_train, X_test, k=3):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "yhat = knn_predict(X_train, y_train, X_test[:20], k=3)\n",
      "check('yhat_shape', yhat.shape == (20,))\n",
      "check('labels', set(np.unique(yhat)).issubset({0,1}))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "### Task 2.2: Evaluate over k\n",
      "Compute accuracy for k in [1,3,5,9,15].\n",
      "\n",
      "**Checkpoint:** Why does increasing k usually increase bias and reduce variance?" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def accuracy(y, yhat):\n",
      "    return float(np.mean(y == yhat))\n",
      "\n",
      "ks = [1,3,5,9,15]\n",
      "for k in ks:\n",
      "    yhat_tr = knn_predict(X_train, y_train, X_train, k=k)\n",
      "    yhat_te = knn_predict(X_train, y_train, X_test, k=k)\n",
      "    print('k', k, 'train_acc', accuracy(y_train, yhat_tr), 'test_acc', accuracy(y_test, yhat_te))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 3 — Curse of dimensionality\n",
      "\n",
      "### Task 3.1: Distance concentration\n",
      "For increasing dimension d, compute ratio min_dist/max_dist for random points and show it approaches 1.\n",
      "\n",
      "# HINT:\n",
      "- sample n points in d dims\n",
      "- compute pairwise distances from one reference point\n" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def concentration_ratio(n=2000, dims=(2,5,10,50,100)):\n",
      "    ratios = []\n",
      "    for d in dims:\n",
      "        X = rng.standard_normal((n, d))\n",
      "        ref = X[0:1]\n",
      "        D = l2_distances(ref, X)[0, 1:]\n",
      "        ratios.append((d, float(D.min() / D.max())))\n",
      "    return ratios\n",
      "\n",
      "ratios = concentration_ratio()\n",
      "for d, r in ratios:\n",
      "    print('d', d, 'min/max', r)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 4 — Feature scaling sensitivity\n",
      "\n",
      "### Task 4.1: Break kNN with scaling\n",
      "Create a dataset where one feature has huge scale and show accuracy drops. Then fix with standardization.\n" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "Xtr, ytr, Xte, yte = make_blobs(d=2)\n",
      "# blow up feature 1 scale\n",
      "Xtr_bad = Xtr.copy(); Xte_bad = Xte.copy()\n",
      "Xtr_bad[:, 1] *= 1000\n",
      "Xte_bad[:, 1] *= 1000\n",
      "\n",
      "yhat_bad = knn_predict(Xtr_bad, ytr, Xte_bad, k=5)\n",
      "acc_bad = accuracy(yte, yhat_bad)\n",
      "print('acc_bad', acc_bad)\n",
      "\n",
      "# TODO: standardize using train mean/std and re-evaluate\n",
      "mu = ...\n",
      "sd = ...\n",
      "Xtr_std = ...\n",
      "Xte_std = ...\n",
      "\n",
      "yhat_std = knn_predict(Xtr_std, ytr, Xte_std, k=5)\n",
      "acc_std = accuracy(yte, yhat_std)\n",
      "print('acc_std', acc_std)\n",
      "check('improves', acc_std >= acc_bad)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "---\n",
      "## Submission Checklist\n",
      "- All TODOs completed\n",
      "- k-sweep results shown\n",
      "- concentration experiment done\n",
      "- scaling fix demonstrated\n"
    ]}
  ]
}
