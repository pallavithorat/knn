{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# kNN & Distance-Based Learning — Student Lab\n",
        "\n",
        "Complete all TODOs. Focus: vectorization, scaling, and failure modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# KNN used in both for classification and regression\n",
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0 — Synthetic dataset generator\n",
        "We’ll generate Gaussian blobs with controllable dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: shapes\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "((400, 5), (200, 5))"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Guassian blob = this is clouds of points drawn from Gaussian distributions\n",
        "def make_blobs(n_train=400, n_test=200, d=2, sep=2.5):\n",
        "    # two Gaussian blobs\n",
        "    mu0 = np.zeros(d)# mu0 = [0,0,0,0,0] , d = 5\n",
        "    mu1 = np.zeros(d); mu1[0] = sep # mu1 = [sep,0,0,0,0]\n",
        "    X0 = rng.standard_normal((n_train//2, d)) + mu0\n",
        "    X1 = rng.standard_normal((n_train - n_train//2, d)) + mu1\n",
        "    X_train = np.vstack([X0, X1])\n",
        "    y_train = np.array([0]*len(X0) + [1]*len(X1))\n",
        "    perm = rng.permutation(n_train)\n",
        "    X_train, y_train = X_train[perm], y_train[perm]\n",
        "\n",
        "    T0 = rng.standard_normal((n_test//2, d)) + mu0\n",
        "    T1 = rng.standard_normal((n_test - n_test//2, d)) + mu1\n",
        "    X_test = np.vstack([T0, T1])\n",
        "    y_test = np.array([0]*len(T0) + [1]*len(T1))\n",
        "    perm = rng.permutation(n_test)\n",
        "    X_test, y_test = X_test[perm], y_test[perm]\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "X_train, y_train, X_test, y_test = make_blobs(d=5)\n",
        "check('shapes', X_train.shape[0] == y_train.shape[0] and X_test.shape[0] == y_test.shape[0])\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1 — Vectorized distances\n",
        "\n",
        "### Task 1.1: L2 distance matrix\n",
        "Compute D where D[i,j] = ||X_test[i] - X_train[j]||_2.\n",
        "\n",
        "# HINT:\n",
        "- Use expansion: ||a-b||^2 = ||a||^2 + ||b||^2 - 2 a·b\n",
        "- Avoid building (m,n,d) broadcast tensor for large sizes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: D_shape\n",
            "OK: D_nonneg\n"
          ]
        }
      ],
      "source": [
        "# vectorization is faster than loops\n",
        "# computing distances of many points at once.\n",
        "# l2 distance = Euclidean distance\n",
        "def l2_distances(X_test, X_train):\n",
        "    # TODO: return (m,n) matrix\n",
        "    a2 = np.sum(X_test * X_test, axis=1, keepdims=True)  # (m,1) \n",
        "    b2 = np.sum(X_train * X_train, axis=1, keepdims=True).T # (1,n)\n",
        "    dist2 = a2 + b2 - 2*(X_test @ X_train.T ) # (m,n)\n",
        "    dist2 = np.maximum(dist2, 0.0)  # numerical stability\n",
        "    return np.sqrt(dist2)\n",
        "\n",
        "D = l2_distances(X_test[:10], X_train[:20])\n",
        "check('D_shape', D.shape == (10, 20))\n",
        "check('D_nonneg', np.all(D >= -1e-9))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: L1 distances (optional)\n",
        "Compute L1 distance matrix (can use broadcasting here since we’ll keep sizes small).\n",
        "\n",
        "**Interview Angle:** When might L1 beat L2?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: D1_shape\n"
          ]
        }
      ],
      "source": [
        "# l1 is Manhattan distance more robust to outliers than l2\n",
        "def l1_distances(X_test, X_train):\n",
        "    # TODO\n",
        "    return np.sum(np.abs(X_test[:, None, :] - X_train[None, :, :]), axis=2)\n",
        "\n",
        "D1 = l1_distances(X_test[:5], X_train[:7])\n",
        "check('D1_shape', D1.shape == (5, 7))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2 — kNN classifier\n",
        "\n",
        "### Task 2.1: Predict with kNN\n",
        "\n",
        "# HINT:\n",
        "- compute distances\n",
        "- argsort distances to find k nearest\n",
        "- majority vote\n",
        "\n",
        "**FAANG gotcha:** define tie-breaking deterministically (e.g., pick smallest label)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK: yhat_shape\n",
            "OK: labels\n"
          ]
        }
      ],
      "source": [
        "def knn_predict(X_train, y_train, X_test, k=3):\n",
        "    # TODO\n",
        "    D = l2_distances(X_test, X_train)  # (m,n)\n",
        "    nn_idx = np.argsort(D, axis=1)[:, :k]  # (m,k) # np.argsort returns indices that would sort an array row-wise\n",
        "    # print(nn_idx.shape)\n",
        "    nn_labels = y_train[nn_idx] \n",
        "    # print(nn_labels.shape)\n",
        "    # majority vote\n",
        "    votes = nn_labels.sum(axis=1) # here votes are assuming binary classification 0 and 1 \n",
        "    return (votes * 2 > k).astype(int) # if votes > k/2 then class 1(Apple) else class 0(Grapes)\n",
        "\n",
        "yhat = knn_predict(X_train, y_train, X_test[:20], k=3)\n",
        "check('yhat_shape', yhat.shape == (20,))\n",
        "check('labels', set(np.unique(yhat)).issubset({0,1}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: Evaluate over k\n",
        "Compute accuracy for k in [1,3,5,9,15].\n",
        "\n",
        "**Checkpoint:** Why does increasing k usually increase bias and reduce variance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k 1 train_acc 1.0 test_acc 0.825\n",
            "k 3 train_acc 0.9325 test_acc 0.87\n",
            "k 5 train_acc 0.905 test_acc 0.885\n",
            "k 9 train_acc 0.89 test_acc 0.88\n",
            "k 15 train_acc 0.885 test_acc 0.89\n"
          ]
        }
      ],
      "source": [
        "def accuracy(y, yhat):\n",
        "    return float(np.mean(y == yhat))\n",
        "\n",
        "ks = [1,3,5,9,15]\n",
        "for k in ks:\n",
        "    yhat_tr = knn_predict(X_train, y_train, X_train, k=k)\n",
        "    yhat_te = knn_predict(X_train, y_train, X_test, k=k)\n",
        "    print('k', k, 'train_acc', accuracy(y_train, yhat_tr), 'test_acc', accuracy(y_test, yhat_te))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3 — Curse of dimensionality\n",
        "\n",
        "### Task 3.1: Distance concentration\n",
        "For increasing dimension d, compute ratio min_dist/max_dist for random points and show it approaches 1.\n",
        "\n",
        "# HINT:\n",
        "- sample n points in d dims\n",
        "- compute pairwise distances from one reference point\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d 2 min/max 0.006214278718774362\n",
            "d 5 min/max 0.10968424369044452\n",
            "d 10 min/max 0.28327247598849514\n",
            "d 50 min/max 0.4947050997185768\n",
            "d 100 min/max 0.6569330655910577\n"
          ]
        }
      ],
      "source": [
        "# demonstrate concentration of distances in high dimensions\n",
        "# distance concentration means that in high-dimensional spaces, the distances between points tend to become similar, making it difficult to distinguish between them based on distance metrics.\n",
        "def concentration_ratio(n=2000, dims=(2,5,10,50,100)):\n",
        "    ratios = []\n",
        "    for d in dims:\n",
        "        X = rng.standard_normal((n, d))\n",
        "        ref = X[0:1] \n",
        "        D = l2_distances(ref, X)[0, 1:]\n",
        "        ratios.append((d, float(D.min() / D.max())))\n",
        "    return ratios\n",
        "\n",
        "ratios = concentration_ratio()\n",
        "for d, r in ratios:\n",
        "    print('d', d, 'min/max', r)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4 — Feature scaling sensitivity\n",
        "\n",
        "### Task 4.1: Break kNN with scaling\n",
        "Create a dataset where one feature has huge scale and show accuracy drops. Then fix with standardization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Xtr, ytr, Xte, yte = make_blobs(d=2)\n",
        "# blow up feature 1 scale\n",
        "Xtr_bad = Xtr.copy(); Xte_bad = Xte.copy()\n",
        "Xtr_bad[:, 1] *= 1000\n",
        "Xte_bad[:, 1] *= 1000\n",
        "\n",
        "yhat_bad = knn_predict(Xtr_bad, ytr, Xte_bad, k=5)\n",
        "acc_bad = accuracy(yte, yhat_bad)\n",
        "print('acc_bad', acc_bad)\n",
        "\n",
        "# TODO: standardize using train mean/std and re-evaluate\n",
        "mu = ...\n",
        "sd = ...\n",
        "Xtr_std = ...\n",
        "Xte_std = ...\n",
        "\n",
        "yhat_std = knn_predict(Xtr_std, ytr, Xte_std, k=5)\n",
        "acc_std = accuracy(yte, yhat_std)\n",
        "print('acc_std', acc_std)\n",
        "check('improves', acc_std >= acc_bad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- k-sweep results shown\n",
        "- concentration experiment done\n",
        "- scaling fix demonstrated\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
